{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# IMPORTACIÓN DE LIBRERÍAS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from PIL import *\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.applications import DenseNet121\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from IPython.display import display\n",
    "#from tensorflow.python.keras import * -> PARA LAS NUEVAS VERSIONES DE TF, HACE QUE NO FUNCIONE\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers, optimizers\n",
    "from keras.applications import ResNet50\n",
    "from keras import backend as K\n",
    "#from keras import optimizers -> PARA LAS NUEVAS VERSIONES DE TF, HACE QUE NO FUNCIONE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import copy\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CARGA DE DATOS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = '/UPC/Tesis/Datasetv4'\n",
    "label_to_text = {0: 'Pterigión grave', 1: 'Pterigión leve', 2: 'Sin pterigión'}\n",
    "data = tf.keras.utils.image_dataset_from_directory(data_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convertir data a tipo lista\n",
    "data_list = list(data.as_numpy_iterator())\n",
    "\n",
    "# Inicializar listas para almacenar temporalmente los datos\n",
    "temp_images = []\n",
    "temp_labels = []\n",
    "\n",
    "for arr in data_list:\n",
    "    temp_images.append(arr[0])  # Añadir imagenes a la lista temporal\n",
    "    temp_labels.append(arr[1])  # Añadir etiquetas a la lista temporal\n",
    "\n",
    "# Concatenar a lo largo de un nuevo eje para obtener un solo arreglo para imágenes y etiquetas respectivamente\n",
    "images = np.concatenate(temp_images, axis=0)\n",
    "labels = np.concatenate(temp_labels, axis=0)\n",
    "\n",
    "temp_images.clear()\n",
    "temp_labels.clear()\n",
    "data_list = []\n",
    "data = None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualización de dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def graphic_plots(columns, images_set, labels_set):\n",
    "    _, ax = plt.subplots(ncols=columns, nrows=columns, figsize=(20,20))\n",
    "    img = images_set[:columns*columns]\n",
    "    for idx in range(columns):\n",
    "        for idy in range(columns):\n",
    "            ax[idx][idy].imshow(img[idx * columns + idy].astype(int))\n",
    "            label_idx = labels_set[idx * columns + idy]\n",
    "            ax[idx][idy].title.set_text(label_to_text[label_idx])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graphic_plots(2, images, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Estadísticas de dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_label_distribution(_labels):\n",
    "    # Contar las ocurrencias de cada etiqueta\n",
    "    unique_labels, counts = np.unique(_labels, return_counts=True)\n",
    "\n",
    "    label_texts = [label_to_text[l] for l in unique_labels]\n",
    "\n",
    "    # Crear un gráfico de torta\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(counts, labels=label_texts, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title('Distribución de etiquetas')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_label_distribution(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AUMENTO DE DATOS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definición de funciones"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gaussian_filter(sigma, num_channels=3):\n",
    "    size = int(6*sigma + 1)\n",
    "    if size % 2 == 0:  # Ensure odd size\n",
    "        size += 1\n",
    "\n",
    "    # Create a 1D Gaussian kernel\n",
    "    x = tf.range(-size // 2 + 1, size // 2 + 1, dtype=tf.float32)\n",
    "    gauss = tf.exp(-tf.pow(x, 2) / (2 * tf.pow(tf.cast(sigma, tf.float32), 2)))\n",
    "    gauss /= tf.reduce_sum(gauss)\n",
    "\n",
    "    # Convert to 2D Gaussian kernel\n",
    "    gauss_kernel = tf.einsum('i,j->ij', gauss, gauss)\n",
    "    gauss_kernel = gauss_kernel[:, :, tf.newaxis, tf.newaxis]\n",
    "    gauss_kernel = tf.tile(gauss_kernel, [1, 1, num_channels, 1])\n",
    "\n",
    "    return gauss_kernel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def augment_image(image, delta, contrast_factor, stddev_noise, sigma_blur, color_correction, gamma, sharpen):\n",
    "    # Ajustar brillo\n",
    "    image = tf.image.adjust_brightness(image, delta=delta)\n",
    "\n",
    "    # Ajustar contraste\n",
    "    image = tf.image.adjust_contrast(image, contrast_factor=contrast_factor)\n",
    "\n",
    "    # Añadir ruido gaussiano\n",
    "    noise = tf.random.normal(shape=tf.shape(image), mean=0., stddev=stddev_noise, dtype=tf.float32)\n",
    "    image = tf.add(image, noise)\n",
    "\n",
    "    # Aplicar desenfoque gaussiano\n",
    "    image = tf.nn.depthwise_conv2d(image[tf.newaxis, ...], gaussian_filter(sigma_blur), strides=[1, 1, 1, 1], padding='SAME')[0]\n",
    "\n",
    "    # Color correction\n",
    "    image *= color_correction\n",
    "\n",
    "    # Gamma correction\n",
    "    image = tf.image.adjust_gamma(image, gamma=gamma)\n",
    "\n",
    "    # Sharpening\n",
    "    no_sharpen = tf.constant([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=tf.float32)[:, :, tf.newaxis, tf.newaxis]\n",
    "    full_sharpen = tf.constant([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]], dtype=tf.float32)[:, :, tf.newaxis, tf.newaxis]\n",
    "    sharpen_filter = sharpen * full_sharpen + (1 - sharpen) * no_sharpen\n",
    "    sharpen_filter = tf.tile(sharpen_filter, [1, 1, 3, 1])\n",
    "    image = tf.nn.depthwise_conv2d(image[tf.newaxis, ...], sharpen_filter, strides=[1, 1, 1, 1], padding='SAME')[0]\n",
    "\n",
    "    # Asegurarse de que la imagen está en el rango [0, 255]\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1er Set\n",
    "modified_images_set_1 = copy.copy(images)\n",
    "modified_images_set_1 = tf.image.convert_image_dtype(modified_images_set_1, dtype=tf.float32)\n",
    "\n",
    "# Ajustes personalizados para la ampliación de datos\n",
    "params = {\n",
    "    \"delta\": -10,  # Ejemplo de ajuste de brillo\n",
    "    \"contrast_factor\": 0.85,  # Ejemplo de ajuste de contraste\n",
    "    \"stddev_noise\": 5,  # Ejemplo de ajuste de ruido\n",
    "    \"sigma_blur\": 1.0,  # Ejemplo de ajuste de desenfoque\n",
    "    \"color_correction\": (1.0, 1.0, 1.2),  # Ajuste de corrección de color (R, G, B)\n",
    "    \"gamma\": 1.05,  # Ajuste de corrección gamma\n",
    "    \"sharpen\": 1.1  # Ajuste de agudización\n",
    "}\n",
    "\n",
    "modified_images_set_1 = tf.map_fn(lambda img: augment_image(img, **params), modified_images_set_1)\n",
    "modified_images_set_1 = (modified_images_set_1.numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2do Set\n",
    "modified_images_set_2 = copy.copy(images)\n",
    "modified_images_set_2 = tf.image.convert_image_dtype(modified_images_set_2, dtype=tf.float32)\n",
    "\n",
    "# Ajustes personalizados para la ampliación de datos\n",
    "params = {\n",
    "    \"delta\": 0,  # Ejemplo de ajuste de brillo\n",
    "    \"contrast_factor\": 0.8,  # Ejemplo de ajuste de contraste\n",
    "    \"stddev_noise\": 5,  # Ejemplo de ajuste de ruido\n",
    "    \"sigma_blur\": 1.5,  # Ejemplo de ajuste de desenfoque\n",
    "    \"color_correction\": (1.2, 1.0, 0.8),  # Ajuste de corrección de color (R, G, B)\n",
    "    \"gamma\": 1.0,  # Ajuste de corrección gamma\n",
    "    \"sharpen\": 1.1  # Ajuste de agudización\n",
    "}\n",
    "\n",
    "modified_images_set_2 = tf.map_fn(lambda img: augment_image(img, **params), modified_images_set_2)\n",
    "modified_images_set_2 = (modified_images_set_2.numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3er Set\n",
    "modified_images_set_3 = copy.copy(images)\n",
    "modified_images_set_3 = tf.image.convert_image_dtype(modified_images_set_3, dtype=tf.float32)\n",
    "\n",
    "# Ajustes personalizados para la ampliación de datos\n",
    "params = {\n",
    "    \"delta\": 50,  # Ejemplo de ajuste de brillo\n",
    "    \"contrast_factor\": 0.7,  # Ejemplo de ajuste de contraste\n",
    "    \"stddev_noise\": 2,  # Ejemplo de ajuste de ruido\n",
    "    \"sigma_blur\": 1.0,  # Ejemplo de ajuste de desenfoque\n",
    "    \"color_correction\": (0.9, 1.0, 1.0),  # Ajuste de corrección de color (R, G, B)\n",
    "    \"gamma\": 0.97,  # Ajuste de corrección gamma\n",
    "    \"sharpen\": 1.5  # Ajuste de agudización\n",
    "}\n",
    "\n",
    "modified_images_set_3 = tf.map_fn(lambda img: augment_image(img, **params), modified_images_set_3)\n",
    "modified_images_set_3 = (modified_images_set_3.numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CONCATENACIÓN\n",
    "augmented_images = np.concatenate((images, modified_images_set_1, modified_images_set_2, modified_images_set_3), axis=0)\n",
    "augmented_labels = np.concatenate((labels, labels, labels, labels), axis=0)\n",
    "modified_images_set_1 = modified_images_set_2 = modified_images_set_3 = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def interpolate_nans(image):\n",
    "    # Crea una matriz de coordenadas para la imagen\n",
    "    x, y = np.indices(image.shape)\n",
    "\n",
    "    # Encuentra los valores NaN en la imagen\n",
    "    nan_mask = np.isnan(image)\n",
    "\n",
    "    # Encuentra los valores no-NaN en la imagen\n",
    "    not_nan_mask = ~nan_mask\n",
    "\n",
    "    # Obtiene las coordenadas y los valores de los píxeles no-NaN\n",
    "    coords_not_nan = np.array([x[not_nan_mask], y[not_nan_mask]]).T\n",
    "    values_not_nan = image[not_nan_mask]\n",
    "\n",
    "    # Interpola para obtener los valores NaN\n",
    "    interpolated_values = griddata(coords_not_nan, values_not_nan, (x[nan_mask], y[nan_mask]), method='nearest')\n",
    "\n",
    "    # Reemplaza los valores NaN en la imagen original\n",
    "    image[nan_mask] = interpolated_values\n",
    "\n",
    "    return image\n",
    "\n",
    "# Para aplicar esto a todas las imágenes en el array 'augmented_images'\n",
    "for i in range(augmented_images.shape[0]):\n",
    "    for c in range(augmented_images.shape[3]):\n",
    "        augmented_images[i, :, :, c] = interpolate_nans(augmented_images[i, :, :, c])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GRAFICAR\n",
    "r = np.random.randint(1, len(images))\n",
    "n = 4\n",
    "\n",
    "temp_list = augmented_images[r:len(augmented_images):len(images)]\n",
    "temp_list_l = augmented_labels[r:len(augmented_labels):len(images)]\n",
    "fig, ax = plt.subplots(ncols=n, nrows=n, figsize=(20, 20))\n",
    "\n",
    "for idx in range(n):\n",
    "    for idy in range(n):\n",
    "        ax[idy][idx].imshow(augmented_images[r + idy + idx * len(images)].astype(int))\n",
    "        ax[idy][idx].title.set_text(augmented_labels[r + idy + idx * len(images)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images = labels = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graphic_plots(2, temp_list, temp_list_l)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def flip_images_by_label(flip_y, flip_x, label, images_set, labels_set):\n",
    "    # Extrae las imágenes que coinciden con la etiqueta deseada\n",
    "    selected_images = images_set[labels_set == label]\n",
    "\n",
    "    # Convierte las imágenes a tensores y normaliza los valores de los píxeles al rango [0, 1]\n",
    "    selected_images = tf.convert_to_tensor(selected_images, dtype=tf.float32) / 255.0\n",
    "\n",
    "    # Aplica los flips según los booleanos flip_x y flip_y\n",
    "    if flip_y:\n",
    "        selected_images = tf.image.flip_left_right(selected_images)\n",
    "    if flip_x:\n",
    "        selected_images = tf.image.flip_up_down(selected_images)\n",
    "\n",
    "    # Ajusta el tono y la saturación\n",
    "    delta_hue = tf.random.uniform(shape=[], minval=-0.05, maxval=0.05)\n",
    "    factor_saturation = tf.random.uniform(shape=[], minval=1.25, maxval=1.75)\n",
    "    selected_images = tf.image.adjust_hue(selected_images, delta_hue)\n",
    "    selected_images = tf.image.adjust_saturation(selected_images, factor_saturation)\n",
    "\n",
    "    # Convierte los valores de los píxeles de vuelta al rango original [0, 255]\n",
    "    selected_images = tf.clip_by_value(selected_images * 255.0, 0, 255)\n",
    "\n",
    "    # Crea un array de etiquetas para las imágenes seleccionadas\n",
    "    selected_labels = np.full((selected_images.shape[0],), label)\n",
    "\n",
    "    return selected_images.numpy(), selected_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aug_images_1, aug_labels_1 = flip_images_by_label(True, False, 2, augmented_images, augmented_labels)\n",
    "aug_images_2, aug_labels_2 = flip_images_by_label(False, True, 2, augmented_images, augmented_labels)\n",
    "aug_images_3, aug_labels_3 = flip_images_by_label(True, True, 2, augmented_images, augmented_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aug_images_4, aug_labels_4 = flip_images_by_label(True, False, 1, augmented_images, augmented_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graphic_plots(4, aug_images_1, aug_labels_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graphic_plots(4, aug_images_2, aug_labels_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graphic_plots(4, aug_images_3, aug_labels_3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graphic_plots(4, aug_images_4, aug_labels_4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CONCATENACIÓN\n",
    "augmented_images = np.concatenate((augmented_images, aug_images_1, aug_images_2, aug_images_3, aug_images_4), axis=0)\n",
    "augmented_labels = np.concatenate((augmented_labels, aug_labels_1, aug_labels_2, aug_labels_3, aug_labels_4), axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aug_images_1 = aug_images_2 = aug_images_3 = aug_images_4 = []\n",
    "aug_labels_1 = aug_labels_2 = aug_labels_3 = aug_labels_4 = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_label_distribution(augmented_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NORMALIZACION DE DATOS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalizar dimensión de imagenes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Se divide el valor de cada pixel por 255 para que contengan valores entre 0 y 1\n",
    "augmented_images = augmented_images/255"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = augmented_images.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalizar tipos de datos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convertimos el tipo array a float32\n",
    "X = np.asarray(X).astype(np.float32)\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = np.eye(3)[augmented_labels]\n",
    "y.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dividir el dataframe en conjunto de entrenamiento, test y validación\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_Test, y_train, y_Test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "X_val, X_Test, y_val, y_Test = train_test_split(X_Test, y_Test, test_size = 0.5, shuffle = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_val.shape, y_val.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_Test.shape, y_Test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def res_block(_X, filter, stage):\n",
    "\n",
    "    # Bloque Convolucional\n",
    "    _X_copy = _X\n",
    "\n",
    "    f1 , f2, f3 = filter\n",
    "\n",
    "    # Camino Principal\n",
    "    _X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_conv_a', kernel_initializer= glorot_uniform(seed = 0))(_X)\n",
    "    _X = MaxPool2D((2,2))(_X)\n",
    "    _X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_a')(_X)\n",
    "    _X = Activation('relu')(_X)\n",
    "\n",
    "    _X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_conv_b', kernel_initializer= glorot_uniform(seed = 0))(_X)\n",
    "    _X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_b')(_X)\n",
    "    _X = Activation('relu')(_X)\n",
    "\n",
    "    _X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_c', kernel_initializer= glorot_uniform(seed = 0))(_X)\n",
    "    _X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_c')(_X)\n",
    "\n",
    "\n",
    "    # Camino Corto\n",
    "    _X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= glorot_uniform(seed = 0))(_X_copy)\n",
    "    _X_copy = MaxPool2D((2,2))(_X_copy)\n",
    "    _X_copy = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_copy')(_X_copy)\n",
    "\n",
    "    # Añadir\n",
    "    _X = Add()([_X,_X_copy])\n",
    "    _X = Activation('relu')(_X)\n",
    "\n",
    "    # Bloque de Identidad 1\n",
    "    _X_copy = _X\n",
    "\n",
    "\n",
    "    # Camino Principal\n",
    "    _X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= glorot_uniform(seed = 0))(_X)\n",
    "    _X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(_X)\n",
    "    _X = Activation('relu')(_X)\n",
    "\n",
    "    _X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= glorot_uniform(seed = 0))(_X)\n",
    "    _X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(_X)\n",
    "    _X = Activation('relu')(_X)\n",
    "\n",
    "    _X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= glorot_uniform(seed = 0))(_X)\n",
    "    _X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_c')(_X)\n",
    "\n",
    "    # Añadir\n",
    "    _X = Add()([_X,_X_copy])\n",
    "    _X = Activation('relu')(_X)\n",
    "\n",
    "    # Bloque de Identidad 2\n",
    "    _X_copy = _X\n",
    "\n",
    "\n",
    "    # Camino Principal\n",
    "    _X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= glorot_uniform(seed = 0))(_X)\n",
    "    _X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(_X)\n",
    "    _X = Activation('relu')(_X)\n",
    "\n",
    "    _X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= glorot_uniform(seed = 0))(_X)\n",
    "    _X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(_X)\n",
    "    _X = Activation('relu')(_X)\n",
    "\n",
    "    _X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= glorot_uniform(seed = 0))(_X)\n",
    "    _X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(_X)\n",
    "\n",
    "    # Añadir\n",
    "    _X = Add()([_X,_X_copy])\n",
    "    _X = Activation('relu')(_X)\n",
    "\n",
    "    return _X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = (256, 256, 3)\n",
    "\n",
    "# Tamaño del tensor de entrada\n",
    "X_input = Input(input_shape)\n",
    "\n",
    "# Zero-padding\n",
    "X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "# 1 - Fase\n",
    "X = Conv2D(64, (7, 7), strides= (2, 2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "X = BatchNormalization(axis =3, name = 'bn_conv1')(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((3, 3), strides= (2, 2))(X)\n",
    "\n",
    "# 2 - Fase\n",
    "X = res_block(X, filter= [64, 64, 256], stage= 2)\n",
    "\n",
    "# 3 - Fase\n",
    "X = res_block(X, filter= [128, 128, 512], stage= 3)\n",
    "\n",
    "# 4 - Fase\n",
    "# X = res_block(X, filter= [256, 256, 1024], stage= 4)\n",
    "\n",
    "# Average Pooling\n",
    "X = AveragePooling2D((4, 4), name = 'Averagea_Pooling')(X)\n",
    "\n",
    "# Capa Final\n",
    "X = Flatten()(X)\n",
    "X = Dense(3, activation = 'softmax', name = 'Dense_final', kernel_initializer= glorot_uniform(seed=0))(X)\n",
    "\n",
    "model_2_emotion = Model(inputs= X_input, outputs = X, name = 'Resnet18')\n",
    "\n",
    "model_2_emotion.summary()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Entrenar la red\n",
    "model_2_emotion.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Recordemos que el primer modelo de puntos faciales clave se guardó con: FacialKeyPoints_weights.hdf5 and FacialKeyPoints-model.json\n",
    "\n",
    "# Usamos la parada temprana para salir del entenamiento si el error de validación\n",
    "# no decrece después de cierto número de epochs (paciencia)\n",
    "earlystopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 20)\n",
    "\n",
    "# Guardamos el mejor modelo con menor error de validación\n",
    "checkpointer = ModelCheckpoint(filepath = \"pterygium_detection.hdf5\", verbose = 1,\n",
    "                               save_best_only=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model_2_emotion.fit(X_train, y_train,\n",
    "                              validation_data=(X_val, y_val), steps_per_epoch=len(X_train) // 64,\n",
    "                              epochs= 20, callbacks=[ checkpointer, earlystopping])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Guardar la arquitectura del modelo en un JSON para su futuro uso\n",
    "\n",
    "model_json = model_2_emotion.to_json()\n",
    "with open(\"detection-model.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('detection-model.json', 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "\n",
    "# Cargamos la arquitectura del modelo\n",
    "model_2_emotion = tf.keras.models.model_from_json(json_savedModel)\n",
    "model_2_emotion.load_weights('pterygium_detection.hdf5')\n",
    "model_2_emotion.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score = model_2_emotion.evaluate(X_Test, y_Test)\n",
    "print('Accuracy en la fase de Test: {}'.format(score[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history.history.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_test = cv2.imread('/UPC/Tesis/Repositories/pteriscope-detection_model/test_grave7.jpg')\n",
    "resize = tf.image.resize(image_test, (256, 256))\n",
    "yhat = model_2_emotion.predict(np.expand_dims(resize/255, 0))\n",
    "yhat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = range(len(accuracy))\n",
    "\n",
    "plt.plot(epochs, accuracy, 'bo', label='Accuracy en el Entrenamiento')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Accuracy en la Validación')\n",
    "plt.title('ACCURACY')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'ro', label='Pérdida en el entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'r', label='Pérdida en la validación')\n",
    "plt.title('LOSS')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predicted_classes = model.predict_classes(X_test)\n",
    "predicted_classes = np.argmax(model_2_emotion.predict(X_Test), axis=-1)\n",
    "y_true = np.argmax(y_Test, axis=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_true.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, predicted_classes)\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.heatmap(cm, annot = True, cbar = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import keras.backend as K\n",
    "\n",
    "def deploy(directory, model):\n",
    "    MODEL_DIR = directory\n",
    "    version = 1\n",
    "\n",
    "    # Juntamos el directorio del temp model con la versión elegida\n",
    "    # El resultado será = '\\tmp\\version number'\n",
    "    export_path = os.path.join(MODEL_DIR, str(version))\n",
    "    print('export_path = {}\\n'.format(export_path))\n",
    "\n",
    "    # Guardemos el modelo con saved_model.save\n",
    "    # Si el directorio existe, debemos borrarlo con '!rm'\n",
    "    # rm elimina cada fichero especificado usando la consola de comandos.\n",
    "\n",
    "    if os.path.isdir(export_path):\n",
    "        print('\\nAlready saved a model, cleaning up\\n')\n",
    "        !rm -r {export_path}\n",
    "\n",
    "    tf.saved_model.save(model, export_path)\n",
    "\n",
    "    os.environ[\"MODEL_DIR\"] = MODEL_DIR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Agreguemos el paquete tensorflow-model-server a nuestra lista de paquetes\n",
    "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
    "    curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
    "!apt update"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
