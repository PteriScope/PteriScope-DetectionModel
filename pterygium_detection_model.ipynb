{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1 CARGAR EL MODELO PRE ENTRENADO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "role = get_execution_role()\n",
    "#role = 'arn:aws:iam::095498312899:role/service-role/AmazonSageMaker-ExecutionRole-20231026T100283';\n",
    "sess = Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "tf_framework_version = tf.__version__\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "from sagemaker.tensorflow.serving import Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imput_bucket = \"pteriscope\"\n",
    "prefix = \"PterygiumDetectionModel\"\n",
    "filename = \"model.zip\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:06:05.817722600Z",
     "start_time": "2023-10-29T00:05:58.302479200Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# if your model is saved as only a .hdf5 file\n",
    "MODEL_LOCATION =''\n",
    "\n",
    "# or if your model is saved as 2 files: model as a .json file, and weights as a .h5 file\n",
    "JSON_LOCATION = 'saved_models/ResNext50/resnext50_model.json'\n",
    "WEIGHTS_LOCATION = 'saved_models/ResNext50/resnext50_weights_.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from JSON_LOCATION and WEIGHTS_LOCATION\n"
     ]
    }
   ],
   "source": [
    "with open(JSON_LOCATION, 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "    json_file.close()\n",
    "\n",
    "# Cargamos la arquitectura del modelo\n",
    "model = tf.keras.models.model_from_json(json_savedModel)\n",
    "model.load_weights(WEIGHTS_LOCATION)\n",
    "model.compile(optimizer = 'SGD', loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "print(\"loaded model from JSON_LOCATION and WEIGHTS_LOCATION\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T18:19:44.960511100Z",
     "start_time": "2023-10-26T18:19:35.046332600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 CONVERTIR EL MODELO AL FORMATO QUE PUEDA LEER AWS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Converts to a Protobuff file\n",
    "- Saves it in a certain aws file structure\n",
    "- Tarballs this file and zips it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: export/Servo/1\\saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "def convert_h5_to_aws(loaded_model):\n",
    "    \"\"\"\n",
    "    given a pre-trained keras model, this function converts it to a TF protobuf format\n",
    "    and saves it in the file structure which aws expects\n",
    "    \"\"\"\n",
    "\n",
    "    from tensorflow.python.saved_model import builder\n",
    "    from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "    # This is the file structure which AWS expects. Cannot be changed.\n",
    "    model_version = '1'\n",
    "    export_dir = 'export/Servo/' + model_version\n",
    "\n",
    "    # Build the Protocol Buffer SavedModel at 'export_dir'\n",
    "    builder = builder.SavedModelBuilder(export_dir)\n",
    "\n",
    "    # Create prediction signature to be used by TensorFlow Serving Predict API\n",
    "    signature = predict_signature_def(\n",
    "        inputs={\"inputs\": loaded_model.input}, outputs={\"score\": loaded_model.output})\n",
    "\n",
    "    from keras import backend as K\n",
    "    with K.get_session() as sess:\n",
    "        # Save the meta graph and variables\n",
    "        builder.add_meta_graph_and_variables(\n",
    "            sess=sess, tags=[tag_constants.SERVING], signature_def_map={\"serving_default\": signature})\n",
    "        builder.save()\n",
    "\n",
    "    #create a tarball/tar file and zip it\n",
    "    import tarfile\n",
    "    with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "        archive.add('export', recursive=True)\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "convert_h5_to_aws(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T04:49:32.887271600Z",
     "start_time": "2023-10-27T04:49:32.575993400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MOVER EL ARCHIVO TARBALL (TAR.GZ) A S3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Acer\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3, re\n",
    "\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session(region_name='us-east-1'))\n",
    "#inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T23:57:26.598708800Z",
     "start_time": "2023-10-28T23:57:25.639566200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket name is:\n"
     ]
    },
    {
     "data": {
      "text/plain": "'sagemaker-us-east-1-095498312899'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where did it upload to?\n",
    "print(\"Bucket name is:\")\n",
    "sagemaker_session.default_bucket()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T23:57:30.299829400Z",
     "start_time": "2023-10-28T23:57:28.331523300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Acer\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# the (default) IAM role you created when creating this notebook\n",
    "role = 'arn:aws:iam::095498312899:role/service-role/AmazonSageMaker-ExecutionRole-20231026T100283'\n",
    "\n",
    "# Create a Sagemaker model (see AWS console>SageMaker>Models)\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',\n",
    "                                  role = role,\n",
    "                                  framework_version = '2.10.0',\n",
    "                                  entry_point = 'train.py')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T23:58:25.023947100Z",
     "start_time": "2023-10-28T23:58:25.008952100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Acer\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n",
      "-----!"
     ]
    }
   ],
   "source": [
    "# Deploy a SageMaker to an endpoint\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.t2.medium')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:01:43.622483300Z",
     "start_time": "2023-10-28T23:58:37.037851300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'tensorflow-inference-2023-10-28-23-58-41-631'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is our endpoint called?\n",
    "endpoint = predictor.endpoint_name\n",
    "endpoint"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:18.567391700Z",
     "start_time": "2023-10-29T00:02:18.542395400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Create a predictor which uses this new endpoint\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "#endpoint = 'tensorflow-inference-2023-10-28-17-25-56-408' #get endpoint name from SageMaker > endpoints\n",
    "\n",
    "endpoint_predictor = sagemaker.tensorflow.model.TensorFlowPredictor(endpoint, sagemaker_session)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:04:43.105454800Z",
     "start_time": "2023-10-29T00:04:43.071438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image_test = cv2.imread('tests/GRAVE.jpg')\n",
    "resize = tf.image.resize(image_test, (224, 224))\n",
    "expanded = np.expand_dims(resize, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:07:17.354165600Z",
     "start_time": "2023-10-29T00:07:17.174171500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{    \"error\": \"Session was not created with a graph before Run()!\"}\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/tensorflow-inference-2023-10-28-23-58-41-631 in account 095498312899 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModelError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m yhat \u001B[38;5;241m=\u001B[39m \u001B[43mpredictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexpanded\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m yhat\n",
      "File \u001B[1;32mF:\\ProgramData\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sagemaker\\tensorflow\\model.py:114\u001B[0m, in \u001B[0;36mTensorFlowPredictor.predict\u001B[1;34m(self, data, initial_args)\u001B[0m\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    112\u001B[0m         args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustomAttributes\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_attributes\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mTensorFlowPredictor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\ProgramData\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sagemaker\\base_predictor.py:188\u001B[0m, in \u001B[0;36mPredictor.predict\u001B[1;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return the inference from the specified endpoint.\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;124;03m        as is.\u001B[39;00m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    180\u001B[0m request_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_request_args(\n\u001B[0;32m    181\u001B[0m     data,\n\u001B[0;32m    182\u001B[0m     initial_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    186\u001B[0m     custom_attributes,\n\u001B[0;32m    187\u001B[0m )\n\u001B[1;32m--> 188\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msagemaker_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msagemaker_runtime_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke_endpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrequest_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_response(response)\n",
      "File \u001B[1;32mF:\\ProgramData\\anaconda3\\envs\\conda_env\\lib\\site-packages\\botocore\\client.py:535\u001B[0m, in \u001B[0;36mClientCreator._create_api_method.<locals>._api_call\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    531\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    532\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpy_operation_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m() only accepts keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    533\u001B[0m     )\n\u001B[0;32m    534\u001B[0m \u001B[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001B[39;00m\n\u001B[1;32m--> 535\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_api_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\ProgramData\\anaconda3\\envs\\conda_env\\lib\\site-packages\\botocore\\client.py:980\u001B[0m, in \u001B[0;36mBaseClient._make_api_call\u001B[1;34m(self, operation_name, api_params)\u001B[0m\n\u001B[0;32m    978\u001B[0m     error_code \u001B[38;5;241m=\u001B[39m parsed_response\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError\u001B[39m\u001B[38;5;124m\"\u001B[39m, {})\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCode\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    979\u001B[0m     error_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mfrom_code(error_code)\n\u001B[1;32m--> 980\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error_class(parsed_response, operation_name)\n\u001B[0;32m    981\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    982\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parsed_response\n",
      "\u001B[1;31mModelError\u001B[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{    \"error\": \"Session was not created with a graph before Run()!\"}\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/tensorflow-inference-2023-10-28-23-58-41-631 in account 095498312899 for more information."
     ]
    }
   ],
   "source": [
    "yhat = predictor.predict(expanded)\n",
    "yhat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:31:52.486992600Z",
     "start_time": "2023-10-29T00:31:47.440995900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
